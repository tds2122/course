{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Topics in Data Science 2*\n",
    "\n",
    "# Descriptive Analytics with Pandas\n",
    "\n",
    "Gunther Gust / Nikolai Stein<br>\n",
    "Chair of Enterprise AI\n",
    "\n",
    "Winter Semester 22/23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Most of the material of this lecture is adopted from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- https://github.com/LearnDataSci/article-resources \n",
    "- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) - Essential Tools for Working with Data\" By Jake VanderPlas,  O'Reilly Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"https://jakevdp.github.io/PythonDataScienceHandbook/figures/PDSH-cover.png\" style=\"width:50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Two ways of exploring data\n",
    "\n",
    "<img src=\"https://github.com/NikoStein/pds_data/raw/main/images/02/explore_data.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "EDA allows to\n",
    "* better understand the data\n",
    "* build an intuition about the data\n",
    "* generate hypotheses\n",
    "* assess assumptions\n",
    "* find insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With EDA we can\n",
    "* get comfortable with the data\n",
    "* find magic features\n",
    "* find mistakes or odd values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Building intuition about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"https://github.com/NikoStein/pds_data/raw/main/images/02/intuitive.png\" style=\"width:80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "+ Is *336* a Typo?\n",
    "+ Do we misinterpret the feature (*age in years*)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing Pandas\n",
    "\n",
    "The *pandas* package is the most important tool at the disposal of Data Scientists and Analysts working in Python today. The powerful machine learning and glamorous visualization tools may get all the attention, but pandas is the backbone of most data projects. \n",
    "\n",
    ">\\[*pandas*\\] is derived from the term \"**pan**el **da**ta\", an econometrics term for data sets that include observations over multiple time periods for the same individuals. — [Wikipedia](https://en.wikipedia.org/wiki/Pandas_%28software%29)\n",
    "\n",
    "We will cover the essential bits of information about pandas, including how to install it, its uses, and how it works with other common Python data analysis packages such as **matplotlib**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Typical Use Cases\n",
    "\n",
    "Pandas has so many uses that it might make sense to list the things it can't do instead of what it can do. \n",
    "\n",
    "For example, say you want to explore a dataset stored in a CSV on your computer. Pandas will extract the data from that CSV into a DataFrame — a table, basically — then let you do things like:\n",
    "\n",
    "- Calculate statistics and answer questions about the data, like\n",
    "    - What's the average, median, max, or min of each column? \n",
    "    - Does column A correlate with column B?\n",
    "    - What does the distribution of data in column C look like?\n",
    "- Clean the data by doing things like removing missing values and filtering rows or columns by some criteria\n",
    "- Visualize the data with help from Matplotlib. Plot bars, lines, histograms, bubbles, and more. \n",
    "- Store the cleaned, transformed data back into a CSV, other file or database\n",
    "\n",
    "Before you jump into predictive or prescriptive modeling you need to have a good understanding of the nature of your dataset and pandas is a great avenue through which to do that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pandas within the data science toolkit\n",
    "\n",
    "Not only is the pandas library a central component of the data science toolkit but it is used in conjunction with other libraries in that collection. \n",
    "\n",
    "Pandas is built on top of the **NumPy** package, meaning a lot of the structure of NumPy is used or replicated in Pandas. Data in pandas is often used to feed statistical analysis in **SciPy**, plotting functions from **Matplotlib**, and machine learning algorithms in **Scikit-learn**.\n",
    "\n",
    "Jupyter Notebooks offer a good environment for using pandas to do data exploration and modeling, but pandas can also be used in text editors just as easily.\n",
    "\n",
    "Jupyter Notebooks give us the ability to execute code in a particular cell as opposed to running the entire file. This saves a lot of time when working with large datasets and complex transformations. Notebooks also provide an easy way to visualize pandas’ DataFrames and plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Installation and Import\n",
    "Pandas is an easy package to install. Open up your terminal program (for Mac users) or command line (for PC users) and install it using either of the following commands:\n",
    "\n",
    "`conda install pandas`\n",
    "\n",
    "OR \n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "Google Colab has pandas pre-installed.\n",
    "\n",
    "To import pandas we usually import it with a shorter name since it's used so much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Series and DataFrames\n",
    "\n",
    "The primary two components of pandas are the `Series` and `DataFrame`. \n",
    "\n",
    "A `Series` is essentially a column vector, and a `DataFrame` is a multi-dimensional table made up of a collection of Series. \n",
    "\n",
    "<img src=\"https://github.com/NikoStein/pds_data/raw/main/images/02/series-and-dataframe.png\" style=\"width:60%\" />\n",
    "\n",
    "DataFrames and Series are quite similar in that many operations that you can do with one you can do with the other, such as filling in null values and calculating the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating DataFrames from scratch\n",
    "\n",
    "Creating DataFrames right in Python is good to know and quite useful when testing new methods and functions you find in the pandas docs. There are *many* ways to create a DataFrame from scratch, but a great option is to just use a simple `dict`. \n",
    "\n",
    "Let's say we are working on a fulfillment project. We want to have a row for each customer and one colum for order and delivery amount each. To organize this as a dictionary for pandas we could do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ordered': [3, 2, 7, 3], \n",
    "    'delivered': [0, 2, 4, 2], \n",
    "}\n",
    "purchases = pd.DataFrame(data)\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Indexing DataFrames\n",
    "\n",
    "Each *(key, value)* item in `data` corresponds to a *column* in the resulting DataFrame. The **Index** of this DataFrame was given to us on creation as the numbers 0-3, but we could also create our own when we initialize the DataFrame. \n",
    "\n",
    "Let's have customer names as our index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases = pd.DataFrame(data, index=['June', 'Robert', 'Lily', 'David'])\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we could **loc**ate a customer's order status by using their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchases.loc['June']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading and writing CSVs\n",
    "It’s quite simple to load data from various file formats into a DataFrame. With CSV files all you need is a single line to load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/NikoStein/pds_data/main/data/purchases.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extensive data preparation you likely want to save it as a file of your choice. Similar to the ways we read in data, pandas provides intuitive commands to save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('new_purchases.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic DataFrame operations: Viewing\n",
    "\n",
    "DataFrames possess hundreds of methods and other operations that are crucial to any analysis. As a beginner, you should know the operations that perform simple transformations of your data and those that provide fundamental statistical analysis.\n",
    "\n",
    "Let's load in the IMDB movies dataset to begin. We're loading this dataset from a CSV and designating the movie titles to be our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(\"https://raw.githubusercontent.com/NikoStein/pds_data/main/data/IMDB-Movie-Data.csv\", index_col=\"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Viewing your data\n",
    "\n",
    "The first thing to do when opening a new dataset is print out a few rows to keep as a visual reference.\n",
    "\n",
    "Typically when we load in a dataset, we like to view the first five or so rows to see what's under the hood. Here we can see the names of each column, the index, and examples of values in each row.\n",
    "\n",
    "`.head()` outputs the **first** five rows of your DataFrame by default, but we could also pass a number as well: `movies_df.head(10)` would output the top ten rows, for example. To see the **last** five rows use `.tail()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Getting info about your data `.info()`\n",
    "\n",
    "`.info()` should be one of the very first commands you run after loading your data. `.info()` provides the essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using. \n",
    "\n",
    "Notice in our movies dataset we have some obvious missing values in the `Revenue` and `Metascore` columns. We'll look at how to handle those in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Getting info about your data: `.shape`\n",
    "\n",
    "Another fast and useful attribute is `.shape`, which outputs just a tuple of (rows, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `.shape` has no parentheses and is a simple tuple of format (rows, columns). So we have **1000 rows** and **11 columns** in our movies DataFrame.\n",
    "\n",
    "The `.shape` command is used a lot when cleaning and transforming data. For example, you might filter some rows based on some criteria and then want to know quickly how many rows were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Describing your variables\n",
    "Using `describe()` on an entire DataFrame we can get a summary of the distribution of continuous variables. Understanding which numbers are continuous also comes in handy when thinking about the type of plot to use to represent your data visually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Describing Categorical Variables\n",
    "`.describe()` can also be used on a categorical variable to get the count of rows, unique count of categories, top category, and freq of top category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Genre'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.value_counts()` can tell us the frequency of all values in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['Genre'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Relationships between continuous variables\n",
    "Examining bivariate relationships comes in handy when you have an outcome or dependent variable in mind and would like to see the features most correlated to the increase or decrease of the outcome. You can visually represent bivariate relationships with scatterplots (seen below in the plotting section). \n",
    "\n",
    "By using the correlation method `.corr()` we can generate the relationship between each continuous variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic DataFrame operations: Data Cleaning\n",
    "\n",
    "### Handling duplicates (1)\n",
    "Duplicate management is key in most settings involving real data. It is a central data intgegration challenge and we want to be able to perform some basic activities using pandas. This dataset does not have duplicate rows, but it is always important to verify you aren't aggregating duplicate rows. \n",
    "\n",
    "To demonstrate, let's simply just double up our movies DataFrame by appending it to itself. Using `append()` will return a copy without affecting the original DataFrame. We are capturing this copy in `temp` so we aren't working with the real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = movies_df.append(movies_df)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try dropping duplicates. Just like `append()`, the `drop_duplicates()` method will also return a copy of your DataFrame, but this time with duplicates removed. Calling `.shape` confirms we're back to the 1000 rows of our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = temp_df.drop_duplicates()\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Handling duplicates (2)\n",
    "\n",
    "Another important argument for `drop_duplicates()` is `keep`, which has three possible options:\n",
    "* `first`: (default) Drop duplicates except for the first occurrence.\n",
    "* `last`: Drop duplicates except for the last occurrence.\n",
    "* `False`: Drop all duplicates.\n",
    "\n",
    "Since we didn't define the `keep` arugment in the previous example it was defaulted to `first`. This means that if two rows are the same pandas will drop the second row and keep the first row. Watch what happens to `temp_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = movies_df.append(movies_df)  # make a new copy\n",
    "temp_df = temp_df.drop_duplicates(keep=False)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Since all rows were duplicates, `keep=False` dropped them all resulting in zero rows being left over. If you're wondering why you would want to do this, one reason is that it allows you to locate all duplicates in your dataset. When conditional selections are shown below you'll see how to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inplace Operations\n",
    "\n",
    "It's a little verbose to keep assigning DataFrames to the same variable like in this example. For this reason, pandas has the `inplace` keyword argument on many of its methods. Using `inplace=True` will modify the DataFrame object in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = movies_df.append(movies_df)  # make a new copy\n",
    "\n",
    "temp_df.drop_duplicates(inplace=True, keep=False)\n",
    "\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Column cleanup\n",
    "\n",
    "Many times datasets will have verbose column names with symbols, upper and lowercase words, spaces, and typos. To make selecting data by column name easier we can spend a little time cleaning up their names. Here's how to print the column names of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only does `.columns` come in handy if you want to rename columns by allowing for simple copy and paste, it's also useful if you need to understand why you are receiving a `Key Error` when selecting data by column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Renaming Columns\n",
    "\n",
    "We can use the `.rename()` method to rename certain or all columns via a `dict`. We don't want parentheses, so let's rename those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.rename(columns={\n",
    "        'Runtime (Minutes)': 'Runtime', \n",
    "        'Revenue (Millions)': 'Revenue_millions'\n",
    "    }, inplace=True)\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. But what if we want to lowercase all names? Instead of using `.rename()` we could also set a list of names to the columns like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.columns = ['rank', 'genre', 'description', 'director', 'actors', 'year', 'runtime', \n",
    "                     'rating', 'votes', 'revenue_millions', 'metascore']\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Operating on many columns\n",
    "With thousands of columns the previous approach is too much work. Instead of just renaming each column manually we can do a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.columns = [col.lower() for col in movies_df.columns]\n",
    "\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`list` (and `dict`) comprehensions come in handy a lot when working with pandas and data in general.\n",
    "\n",
    "It's a good idea to lowercase, remove special characters, and replace spaces with underscores if you'll be working with a dataset for some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Working with missing values\n",
    "\n",
    "When exploring data, you’ll most likely encounter missing or null values, which are essentially placeholders for non-existent values. Most commonly you'll see Python's `None` or NumPy's `np.nan`, each of which are handled differently in some situations.\n",
    "\n",
    "There are two options in dealing with nulls: \n",
    "\n",
    "1. Get rid of rows or columns with nulls\n",
    "2. Replace nulls with non-null values, a technique known as **imputation**\n",
    "\n",
    "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our DataFrame are null:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identify missing values\n",
    "`isnull()` returns a DataFrame where each cell is either True or False depending on that cell's null status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isnull().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.isnull()` just by iteself isn't very useful, and is usually used in conjunction with other methods, like `sum()`. To count the number of nulls in each column we use an aggregate function for summing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Removing null values\n",
    "\n",
    "Data Scientists and Analysts regularly face the dilemma of dropping or imputing null values, and is a decision that requires intimate knowledge of your data and its context. Overall, removing null data is only suggested if you have a small amount of missing data.\n",
    "\n",
    "Remove nulls is pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = movies_df.dropna()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This operation deletes any **row** with at least a single null value. In our case, it removes 128 rows where `revenue_millions` is null and 64 rows where `metascore` is null. This obviously seems like a waste since there's perfectly good data in the other columns of those dropped rows. That's why we'll look at imputation next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Imputation\n",
    "\n",
    "Imputation is a conventional feature engineering technique used to keep valuable data that have null values. There may be instances where dropping every row with a null value removes too big a chunk from your dataset, so instead we can impute that null with another value, usually the **mean** or the **median** of that column. \n",
    "\n",
    "Let's look at imputing the missing values in the `revenue_millions` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue = movies_df['revenue_millions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using square brackets is the general way we select columns in a DataFrame. `revenue` is a Series and we can calculate its mean and fill the nulls using `fillna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revenue_mean = revenue.mean()\n",
    "print(revenue_mean)\n",
    "revenue.fillna(revenue_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We have now replaced all nulls in `revenue` with the mean of the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing an entire column with the same value like this is a basic example. It would be a better idea to try a more granular imputation by Genre or Director. For example, you would find the mean of the revenue generated in each genre individually and impute the nulls in each genre with that genre's mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic DataFrame operations: Slicing, selecting, extracting\n",
    "\n",
    "Up until now we've focused on some basic summaries of our data. We've learned about simple column extraction using single brackets, and we imputed null values in a column using `fillna()`. Below are the other methods of slicing, selecting, and extracting you'll need to use constantly.\n",
    "\n",
    "It's important to note that, although many methods are the same, DataFrames and Series have different attributes, so you'll need be sure to know which type you are working with or else you will receive attribute errors. \n",
    "\n",
    "Let's look at working with columns first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### By column\n",
    "\n",
    "You already saw how to extract a column using square brackets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_col = movies_df['genre']\n",
    "type(genre_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return a *Series*. To extract a column as a *DataFrame*, you need to pass a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_col = movies_df[['genre']]\n",
    "type(genre_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's just a list, adding another column name is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = movies_df[['genre', 'rating']]\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### By row\n",
    "For rows, we have two options: \n",
    "- `.loc` - **loc**ates by name\n",
    "- `.iloc`- **loc**ates by numerical **i**ndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom = movies_df.loc[\"Prometheus\"]\n",
    "prom[[\"description\",\"director\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `iloc` we give it the numerical index of Prometheus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prom = movies_df.iloc[1]\n",
    "prom[[\"description\",\"director\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conditional selections\n",
    "We’ve gone over how to select columns and rows, but what if we want to make a conditional selection? \n",
    "\n",
    "For example, what if we want to filter our movies DataFrame to show only films directed by Ridley Scott or films with a rating greater than or equal to 8.0?\n",
    "\n",
    "To do that, we take a column from the DataFrame and apply a Boolean condition to it. Here's an example of a Boolean condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df['director'] == \"Ridley Scott\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combining conditions\n",
    "We can make some richer conditionals by using logical operators `|` for \"or\" and `&` for \"and\".\n",
    "\n",
    "Let's filter the the DataFrame to show only movies by Christopher Nolan OR Ridley Scott:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[(movies_df['director'] == 'Christopher Nolan') | (movies_df['director'] == 'Ridley Scott')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combining conditions (2)\n",
    "We need to make sure to group evaluations with parentheses so Python knows how to evaluate the conditional.\n",
    "\n",
    "`(movies_df['director'] == 'Christopher Nolan') | (movies_df['director'] == 'Ridley Scott')`\n",
    "\n",
    "Using the `isin()` method we could make this more concise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['Christopher Nolan', 'Ridley Scott']\n",
    "movies_df[movies_df['director'].isin(selection)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combining conditions (3)\n",
    "Let's say we want all movies that were released between 2005 and 2010, have a rating above 8.0, but made below the 25th percentile in revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\n",
    "    ((movies_df['year'] >= 2005) \n",
    "    & (movies_df['year'] <= 2010))\n",
    "    & (movies_df['rating'] > 8.0)\n",
    "    & (movies_df['revenue_millions'] < movies_df['revenue_millions'].quantile(0.25))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aggregation and Grouping\n",
    "An essential piece of analysis of large data is efficient summarization: computing aggregations like ``sum()``, ``mean()``, ``median()``, ``min()``, and ``max()``, in which a single number gives insight into the nature of a potentially large dataset.\n",
    "In this section, we'll explore aggregations in Pandas, from simple operations akin to what we've seen on NumPy arrays, to more sophisticated operations based on the concept of a ``groupby``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following table summarizes some other built-in Pandas aggregations:\n",
    "\n",
    "| Aggregation              | Description                     |\n",
    "|--------------------------|---------------------------------|\n",
    "| ``count()``              | Total number of items           |\n",
    "| ``first()``, ``last()``  | First and last item             |\n",
    "| ``mean()``, ``median()`` | Mean and median                 |\n",
    "| ``min()``, ``max()``     | Minimum and maximum             |\n",
    "| ``std()``, ``var()``     | Standard deviation and variance |\n",
    "| ``mad()``                | Mean absolute deviation         |\n",
    "| ``prod()``               | Product of all items            |\n",
    "| ``sum()``                | Sum of all items                |\n",
    "\n",
    "These are all methods of ``DataFrame`` and ``Series`` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's calculate some statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "movies_df.std(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "movies_df['revenue_millions'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### GroupBy: Split, Apply, Combine\n",
    "Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so-called groupby operation. The name \"group by\" comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine.\n",
    "\n",
    "<img src=\"https://github.com/jakevdp/PythonDataScienceHandbook/raw/master/notebooks/figures/03.08-split-apply-combine.png\" style=\"width:60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Column indexing\n",
    "\n",
    "The ``GroupBy`` object supports column indexing in the same way as the ``DataFrame``, and returns a modified ``GroupBy`` object.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.groupby('genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "movies_df.groupby('genre')['revenue_millions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "movies_df.groupby('genre')['revenue_millions'].median().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying functions\n",
    "\n",
    "It is possible to iterate over a DataFrame or Series as you would with a list, but doing so — especially on large datasets — is very slow.\n",
    "\n",
    "An efficient alternative is to `apply()` a function to the dataset. For example, we could use a function to convert movies with an 8.0 or greater to a string value of \"good\" and the rest to \"bad\" and use this transformed values to create a new column.\n",
    "\n",
    "First we would create a function that, when given a rating, determines if it's good or bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_function(x):\n",
    "    if x >= 8.0:\n",
    "        return \"good\"\n",
    "    else:\n",
    "        return \"bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we want to send the entire rating column through this function, which is what `apply()` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[\"rating_category\"] = movies_df[\"rating\"].apply(rating_function)\n",
    "movies_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides being much more concise than a loop structure, using `apply()` will also be much faster than iterating manually over rows because pandas is utilizing vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plotting\n",
    "\n",
    "Another great thing about pandas is that it integrates with Matplotlib, so you get the ability to plot directly off DataFrames and Series. To get started we need to import Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Plotting from an IPython notebook\n",
    "\n",
    "Plotting interactively within an IPython notebook can be done with the ``%matplotlib`` command, and works in a similar way to the IPython shell.\n",
    "In the IPython notebook, you also have the option of embedding graphics directly in the notebook, with two possible options:\n",
    "\n",
    "- ``%matplotlib notebook`` will lead to *interactive* plots embedded within the notebook\n",
    "- ``%matplotlib inline`` will lead to *static* images of your plot embedded in the notebook\n",
    "\n",
    "We will generally opt for ``%matplotlib inline``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A simple plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(x, np.sin(x), '-')\n",
    "plt.plot(x, np.cos(x), '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Matplotlib options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Matplotlib from within a script, the function ``plt.show()`` is your friend.\n",
    "``plt.show()`` starts an event loop, looks for all currently active figure objects, and opens one or more interactive windows that display your figure or figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20, 'figure.figsize': (10, 8)}) # set font and plot size to be larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin. There won't be a lot of coverage on plotting, but it should be enough to explore you're data easily.\n",
    "\n",
    "**Side note:**\n",
    "For categorical variables utilize Bar Charts* and Boxplots.  For continuous variables utilize Histograms, Scatterplots, Line graphs, and Boxplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scatterplot\n",
    "\n",
    "Let's plot the relationship between ratings and revenue. All we need to do is call `.plot()` on `movies_df` with some info about how to construct the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.plot(kind='scatter', x='rating', y='revenue_millions', title='Revenue (millions) vs Rating');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semicolon is not a syntax error, just a way to hide extra output in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Histogram\n",
    "If we want to plot a simple Histogram based on a single column, we can call plot on a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df['rating'].plot(kind='hist', title='Rating', bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Boxplot\n",
    "Using a Boxplot we can visualize the rating quartiles of the n directors with the most movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "directors = movies_df['director'].value_counts()[:n].index.tolist()\n",
    "chart = movies_df[movies_df['director'].isin(directors)].boxplot(column=['rating'], by=\"director\")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wrapping up\n",
    "\n",
    "Exploring, cleaning, transforming, and visualization data with pandas in Python is an essential skill in data science. Just cleaning wrangling data is 80% of your job as a Data Scientist. After a few projects and some practice, you should be very comfortable with most of the basics.\n",
    "\n",
    "To keep improving, view the [extensive tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html) offered by the official pandas docs, follow along with a few [Kaggle kernels](https://www.kaggle.com/kernels), and keep working on your own projects!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'>WS 22/23</br>TDS 2</div><div class='logo'><img src='images/unilogo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
